{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e7c831",
   "metadata": {},
   "source": [
    "# Sistemas inteligentes. Aplicaciones\n",
    "\n",
    "## Práctica 2-2. Preprocesamiento 2\n",
    "\n",
    "En esta práctica vas a unir todas las técnicas que hemos visto hasta ahora, para trabajar en un problema de clasificación de textos.\n",
    "\n",
    "Para realizar esta práctica vamos a utilizar un dataset que contiene documentos de texto ofrecido por scikit_learn. Este dataset se llama [Twenty Newsgroups](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups). Su descripción oficial es la siguiente: \n",
    "\n",
    "    \"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\n",
    "\n",
    "Para trabajar con este dataset lo primero que debemos hacer es importarlo:\n",
    "\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "Una vez importado se leerán los datos de los documentos. Para ello, la llamada al constructor de la clase es la siguiente:, se establece como subset la opción train y de esta forma devuelve los datos de entrenamiento (poner esta opción a test para leer los datos de test).\n",
    "\n",
    "    twenty_train = fetch_20newsgroups(subset=tipoDatos, shuffle=aleatorio, random_state=semilla, categories=clasesDocumentos)\n",
    "\n",
    "Los parámetros son los siguientes:\n",
    "* tipoDatos: string que determina si los datos son de entrenamiento (asignar 'train') o de test (asignar 'test')\n",
    "* aleatorio: valor booleano que establece si se aleatorizan los datos o no.\n",
    "* semilla: valor entero que determina la semilla para la generación de números aleatorios. De esta forma los experimentos serán reproducibles.\n",
    "* clasesDocumentos: lista con los nombre de las clases de los documentos a leer. Si se asigna a None se leen los documentos de todas las clases.\n",
    "\n",
    "El objeto generado (variable twenty_train) tiene la misma estructura que todos los datasets nativos de Scikit-learn con los que hemos trabajado. Por tanto:\n",
    "* Los **datos de entrada** correspondientes a los documentos de texto se encuentran en el campo **data**. Por ejemplo, si se desea mostrar el primer documento se puede ejecutar:\tprint(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))\n",
    "* Los **nombres de las clases** se encuentran en el campo **target_names**. En este corpus de documentos se encuentran documentos clasificados en 20 clases diferentes, para visualizarlos ejecuta: print twenty_train.target_names\n",
    "* Las **clases** de cada documento se encuentran en el campo **target**. Por ejemplo si se quiere obtener la clase del primer documento se ejecutaría: print twenty_train.target[0]\n",
    "* Si anidamos los campos target_names y target podemos visualizar la clase de cada documento. Por ejemplo si se desea conocer la clase del primer documento se ejecutaría: print twenty_train.target_names[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7418e5a",
   "metadata": {},
   "source": [
    "Para simplificar el problema, no vamos a trabajar con todas las clases que existen en el dataset. por contra, solo vamos a quedarnos con las clases: 'alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med'.\n",
    "\n",
    "Para los datos de dichas clases, debes ser capaz de transformarlos en un dataset tabular y entrenar un modelo de clasificación supervisada que aprenda a determinar la clase de un documento. \n",
    "\n",
    "Los pasos a seguir con los vistos en clase hasta el momento:\n",
    "* preprocesamiento simple del texto\n",
    "* tokenización\n",
    "* stemming / lemmatization\n",
    "* bag of words\n",
    "* aprender modelo de clasificación (naive-bayes)\n",
    "* obtener accuracy en train y test\n",
    "\n",
    "En primer lugar, debes ser capaz de montar un proceso completo de clasificación, desde la lectura de los datos hasta la obtención del accuracy del modelo. Una vez ue tengas un modelo completo funcionando, puedes ir cambiando los diferentes parámetros, para intentar obtener el máximo de accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a148ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/juancho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/juancho/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/juancho/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos de entrenamiento: 2257\n",
      "Documentos de test: 1502\n",
      "Categorías: ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "\n",
      "Ejemplo documento original (primeras 200 chars):\n",
      "From: jr0930@eve.albany.edu (REGAN JAMES P)\n",
      "Subject: Re: Pascal-Fractals\n",
      "Organization: State University of New York at Albany\n",
      "Lines: 10\n",
      "\n",
      "Apparently, my editor didn't do what I wanted it to do, so I'll\n",
      "\n",
      "Ejemplo documento procesado (primeras 200 chars):\n",
      "from jr0930evealbanyedu regan jame p subject re pascalfract organ state univers of new york at albani line 10 appar my editor didnt do what i want it to do so ill tri again im look for ani program or \n",
      "\n",
      "Tamaño vocabulario: 30571\n",
      "Matriz train: (2257, 30571)\n",
      "Matriz test: (1502, 30571)\n",
      "\n",
      "Accuracy en TRAIN: 0.9938\n",
      "Accuracy en TEST:  0.9201\n",
      "\n",
      "Classification Report (Test):\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.88      0.87      0.88       319\n",
      "soc.religion.christian       0.95      0.94      0.94       389\n",
      "         comp.graphics       0.94      0.91      0.92       396\n",
      "               sci.med       0.90      0.95      0.93       398\n",
      "\n",
      "              accuracy                           0.92      1502\n",
      "             macro avg       0.92      0.92      0.92      1502\n",
      "          weighted avg       0.92      0.92      0.92      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- 1. Carga de datos ---\n",
    "seed = 12\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=seed, categories=categories)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=seed, categories=categories)\n",
    "\n",
    "print(f\"Documentos de entrenamiento: {len(twenty_train.data)}\")\n",
    "print(f\"Documentos de test: {len(twenty_test.data)}\")\n",
    "print(f\"Categorías: {twenty_train.target_names}\")\n",
    "\n",
    "# --- 2. Preprocesamiento + Tokenización + Stemming ---\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Eliminar caracteres no alfanuméricos (excepto espacios)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(text)\n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocesar todos los documentos de train y test\n",
    "train_processed = [preprocess(doc) for doc in twenty_train.data]\n",
    "test_processed = [preprocess(doc) for doc in twenty_test.data]\n",
    "\n",
    "print(f\"\\nEjemplo documento original (primeras 200 chars):\\n{twenty_train.data[0][:200]}\")\n",
    "print(f\"\\nEjemplo documento procesado (primeras 200 chars):\\n{train_processed[0][:200]}\")\n",
    "\n",
    "# --- 3. Bag of Words ---\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_processed)\n",
    "X_test = vectorizer.transform(test_processed)\n",
    "\n",
    "print(f\"\\nTamaño vocabulario: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Matriz train: {X_train.shape}\")\n",
    "print(f\"Matriz test: {X_test.shape}\")\n",
    "\n",
    "# --- 4. Modelo Naive-Bayes ---\n",
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --- 5. Accuracy en train y test ---\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nAccuracy en TRAIN: {acc_train:.4f}\")\n",
    "print(f\"Accuracy en TEST:  {acc_test:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (Test):\\n\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=categories))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
